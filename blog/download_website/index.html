<!DOCTYPE html>
<html>
<head lang="ru">
    <meta charset="UTF-8">
    
    <title>Как выкачать сайт. wget</title>
    <link rel="stylesheet" href="/static/css/post.css"/>


    <!-- stylesheets -->
    <link rel="stylesheet" href="/static/css/base.css"/>
    <link rel="stylesheet" href="/static/css/syntax.css"/>
    <!-- atom feed -->
    <link href="/feed.atom"
      rel="alternate"
      title="Recent Changes"
      type="application/atom+xml">
</head>
<body>

    
<!-- header section -->
<header>
  <a style="border-style: none" href="/"><img src="/static/img/arrow_l_32x32.png" alt="назад"></a>
  <h1>Как выкачать сайт. wget</h1>
    
    <h3 id="date">28-02-2015</h3>
</header>

<!-- content section -->
<div id="content">
    <p>Для выкачки сайта при помощи wget над использовать сл ключи:</p>
<div class="codehilite"><pre>wget -r -k -l <span class="m">7</span> -p -E -nc http://site.com/
</pre></div>


<p>-r  —    указывает на то, что нужно рекурсивно переходить по ссылкам на сайте, чтобы скачивать страницы.</p>
<p>-k  —    используется для того, чтобы wget преобразовал все ссылки в скаченных файлах таким образом, чтобы по ним можно было переходить на локальном компьютере (в автономном режиме).</p>
<p>-p  —    указывает на то, что нужно загрузить все файлы, которые требуются для отображения страниц (изображения, css и т.д.).</p>
<p>-l  —    определяет максимальную глубину вложенности страниц, которые wget должен скачать (по умолчанию значение равно 5, в примере мы установили 7). В большинстве случаев сайты имеют страницы с большой степенью вложенности и wget может просто «закопаться», скачивая новые страницы. Чтобы этого не произошло можно использовать параметр -l.</p>
<p>-E  —    добавлять к загруженным файлам расширение .html.</p>
<p>-nc —    при использовании данного параметра существующие файлы не будут перезаписаны. Это удобно, когда нужно продолжить загрузку сайта, прерванную в предыдущий раз.</p>
<p>--no-parent - не скачивать выше указанного каталога/ссылки.</p>
<p>стырено <a href="http://ru.najomi.org/_nix/wget">отсюда</a></p>
<p>Некоторые сайты, например мне попался один вики-сайт, который таким образом выкачиватся не желал. Для такого сработала такая комбинация ключей:</p>
<div class="codehilite"><pre>wget -k -p -r -l <span class="m">1</span> --restrict-file-names<span class="o">=</span>nocontrol,windows --html-extension <span class="se">\</span>
-e <span class="nv">robots</span><span class="o">=</span>off http://tralala.com
</pre></div>


<p>Скорее всего это связано с шаблоном ссылок внутри сайта, http://.../index.php@some_page, но я не проверял.</p>
</div>


    <br />
    <footer>
      <a href="/feed.atom"><img src="/static/img/rss_32x32.png" alt="rss"></a>
      <a href="https://plus.google.com/u/0/113483139834962824849"><img src="/static/img/google_32x32.png" alt="google plus"></a>
      <a href="#top"><img src="/static/img/arrow_up_32x32.png" alt="наверх"></a>
    </footer>
</body>
</html>