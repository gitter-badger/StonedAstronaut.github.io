title: Как выкачать сайт. wget
date: 2015-02-28
published: True

Для выкачки сайта при помощи wget над использовать сл ключи:

    :::bash
    wget -r -k -l 7 -p -E -nc http://site.com/

-r  —	 указывает на то, что нужно рекурсивно переходить по ссылкам на сайте, чтобы скачивать страницы.

-k  —	 используется для того, чтобы wget преобразовал все ссылки в скаченных файлах таким образом, чтобы по ним можно было переходить на локальном компьютере (в автономном режиме).

-p  —	 указывает на то, что нужно загрузить все файлы, которые требуются для отображения страниц (изображения, css и т.д.).

-l  —	 определяет максимальную глубину вложенности страниц, которые wget должен скачать (по умолчанию значение равно 5, в примере мы установили 7). В большинстве случаев сайты имеют страницы с большой степенью вложенности и wget может просто «закопаться», скачивая новые страницы. Чтобы этого не произошло можно использовать параметр -l.

-E  —	 добавлять к загруженным файлам расширение .html.

-nc —	 при использовании данного параметра существующие файлы не будут перезаписаны. Это удобно, когда нужно продолжить загрузку сайта, прерванную в предыдущий раз.

--no-parent - не скачивать выше указанного каталога/ссылки.

стырено [отсюда](http://ru.najomi.org/_nix/wget)

Некоторые сайты, например мне попался один вики-сайт, который таким образом выкачиватся не желал. Для такого сработала такая комбинация ключей:

	:::bash
	wget -k -p -r -l 1 --restrict-file-names=nocontrol,windows --html-extension -e robots=off http://tralala.com

Скорее всего это связано с шаблоном ссылок внутри сайта, http://.../index.php@some_page, но я не проверял.